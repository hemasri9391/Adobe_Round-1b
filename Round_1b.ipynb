{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hemasri9391/Adobe_Round-1b/blob/main/Round_1b.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "328BfWMedllk",
        "outputId": "1fd1d748-168e-43e1-8a57-51038fd8edb3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyMuPDF\n",
            "  Downloading pymupdf-1.26.3-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n",
            "Downloading pymupdf-1.26.3-cp39-abi3-manylinux_2_28_x86_64.whl (24.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyMuPDF\n",
            "Successfully installed PyMuPDF-1.26.3\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "!pip install PyMuPDF\n",
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4d3bBkbaflDy"
      },
      "outputs": [],
      "source": [
        "import fitz\n",
        "import requests\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from datetime import datetime\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-ipBpIIi6QJ",
        "outputId": "c62de7b8-7046-413b-ab2c-0ff8d08c9d3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m225.3/232.6 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_urls = [\n",
        "    \"https://arxiv.org/pdf/2103.09430.pdf\",\n",
        "    \"https://arxiv.org/pdf/2204.08706.pdf\",\n",
        "    \"https://arxiv.org/pdf/2006.01460.pdf\"\n",
        "]\n",
        "\n",
        "pdf_docs = []\n",
        "for idx, url in enumerate(pdf_urls):\n",
        "    response = requests.get(url)\n",
        "    filename = f\"paper_{idx+1}.pdf\"\n",
        "    with open(filename, \"wb\") as f:\n",
        "        f.write(response.content)\n",
        "    pdf_docs.append(filename)\n",
        "print(\"All PDFs downloaded successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1U_v-H7Fgb7A",
        "outputId": "b432990c-9760-46c0-9a86-515bd9c7bb14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All PDFs downloaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Markdown\n",
        "import requests\n",
        "from PyPDF2 import PdfReader\n",
        "from io import BytesIO\n",
        "def extract_title_from_url(url):\n",
        "    response = requests.get(url)\n",
        "    reader = PdfReader(BytesIO(response.content))\n",
        "    first_page = reader.pages[0]\n",
        "    text = first_page.extract_text()\n",
        "    lines = [line.strip() for line in text.split('\\n') if line.strip()]\n",
        "    title_candidates = [line for line in lines if 20 < len(line) < 1000]\n",
        "    title = title_candidates[0] if title_candidates else \"Title not found\"\n",
        "    return title\n",
        "\n",
        "display(Markdown(\"**Extracted Research Paper Titles**\"))\n",
        "\n",
        "for idx, url in enumerate(pdf_urls):\n",
        "    title = extract_title_from_url(url)\n",
        "    display(Markdown(f\"Paper {idx + 1}:\\n **{title}**\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116
        },
        "id": "O5Y-mP7PjSIn",
        "outputId": "66acfbfc-2b1d-4f66-ddd3-c587161a5a3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Extracted Research Paper Titles**"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Paper 1:\n **OGB-LSC: A Large-Scale Challenge for**"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Paper 2:\n **Recycling End–of–life**"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Paper 3:\n **Situated and Interactive Multimodal Conversations**"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_sections_from_pdf(file_path):\n",
        "    doc = fitz.open(file_path)\n",
        "    extracted = []\n",
        "\n",
        "    for page_num, page in enumerate(doc, start=1):\n",
        "        text = page.get_text()\n",
        "        if any(keyword in text.lower() for keyword in [\"method\", \"dataset\", \"benchmark\"]):\n",
        "            extracted.append({\n",
        "                \"document\": file_path,\n",
        "                \"page_number\": page_num,\n",
        "                \"section_title\": f\"Possible Relevant Section Page {page_num}\",\n",
        "                \"importance_rank\": 1 if \"method\" in text.lower() else 2\n",
        "            })\n",
        "    return extracted\n",
        "\n",
        "all_extracted_sections = []\n",
        "for pdf in pdf_docs:\n",
        "    extracted = extract_sections_from_pdf(pdf)\n",
        "    all_extracted_sections.extend(extracted)\n",
        "print(f\"Extracted {len(all_extracted_sections)} relevant sections.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oBZvJXHNghRx",
        "outputId": "0760a74d-82d5-44e3-dd42-3b47a0dab514"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted 40 relevant sections.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "def get_refined_text(file_path, page_num):\n",
        "    doc = fitz.open(file_path)\n",
        "    page_text = doc[page_num - 1].get_text()\n",
        "    sentences = sent_tokenize(page_text)\n",
        "    refined = \" \".join(sentences[:3])  # taking first 3 sentences for demo\n",
        "    return refined\n",
        "\n",
        "subsection_analysis = []\n",
        "for section in all_extracted_sections:\n",
        "    refined_text = get_refined_text(section[\"document\"], section[\"page_number\"])\n",
        "    subsection_analysis.append({\n",
        "        \"document\": section[\"document\"],\n",
        "        \"page_number\": section[\"page_number\"],\n",
        "        \"refined_text\": refined_text\n",
        "    })\n",
        "\n",
        "print(f\"Sub-section analysis for {len(subsection_analysis)} sections done.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zq8IxB6-goFb",
        "outputId": "faacdc29-5018-4323-9aa3-156e1e70a64b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sub-section analysis for 40 sections done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = {\n",
        "    \"metadata\": {\n",
        "        \"input_documents\": pdf_docs,\n",
        "        \"persona\": \"PhD Researcher in Computational Biology\",\n",
        "        \"job_to_be_done\": \"Prepare a comprehensive literature review focusing on methodologies, datasets, and performance benchmarks\",\n",
        "        \"processing_timestamp\": datetime.now().isoformat()\n",
        "    },\n",
        "    \"extracted_sections\": all_extracted_sections,\n",
        "    \"subsection_analysis\": subsection_analysis\n",
        "}\n",
        "print(json.dumps(output, indent=2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IMg8IL8Vgr4j",
        "outputId": "0801db9f-d3ad-4520-a4f5-dc8eb4720e6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"metadata\": {\n",
            "    \"input_documents\": [\n",
            "      \"paper_1.pdf\",\n",
            "      \"paper_2.pdf\",\n",
            "      \"paper_3.pdf\"\n",
            "    ],\n",
            "    \"persona\": \"PhD Researcher in Computational Biology\",\n",
            "    \"job_to_be_done\": \"Prepare a comprehensive literature review focusing on methodologies, datasets, and performance benchmarks\",\n",
            "    \"processing_timestamp\": \"2025-07-22T14:22:10.808977\"\n",
            "  },\n",
            "  \"extracted_sections\": [\n",
            "    {\n",
            "      \"document\": \"paper_1.pdf\",\n",
            "      \"page_number\": 1,\n",
            "      \"section_title\": \"Possible Relevant Section Page 1\",\n",
            "      \"importance_rank\": 2\n",
            "    },\n",
            "    {\n",
            "      \"document\": \"paper_1.pdf\",\n",
            "      \"page_number\": 2,\n",
            "      \"section_title\": \"Possible Relevant Section Page 2\",\n",
            "      \"importance_rank\": 1\n",
            "    },\n",
            "    {\n",
            "      \"document\": \"paper_1.pdf\",\n",
            "      \"page_number\": 3,\n",
            "      \"section_title\": \"Possible Relevant Section Page 3\",\n",
            "      \"importance_rank\": 2\n",
            "    },\n",
            "    {\n",
            "      \"document\": \"paper_1.pdf\",\n",
            "      \"page_number\": 4,\n",
            "      \"section_title\": \"Possible Relevant Section Page 4\",\n",
            "      \"importance_rank\": 1\n",
            "    },\n",
            "    {\n",
            "      \"document\": \"paper_1.pdf\",\n",
            "      \"page_number\": 5,\n",
            "      \"section_title\": \"Possible Relevant Section Page 5\",\n",
            "      \"importance_rank\": 2\n",
            "    },\n",
            "    {\n",
            "      \"document\": \"paper_1.pdf\",\n",
            "      \"page_number\": 6,\n",
            "      \"section_title\": \"Possible Relevant Section Page 6\",\n",
            "      \"importance_rank\": 2\n",
            "    },\n",
            "    {\n",
            "      \"document\": \"paper_1.pdf\",\n",
            "      \"page_number\": 7,\n",
            "      \"section_title\": \"Possible Relevant Section Page 7\",\n",
            "      \"importance_rank\": 2\n",
            "    },\n",
            "    {\n",
            "      \"document\": \"paper_1.pdf\",\n",
            "      \"page_number\": 8,\n",
            "      \"section_title\": \"Possible Relevant Section Page 8\",\n",
            "      \"importance_rank\": 2\n",
            "    },\n",
            "    {\n",
            "      \"document\": \"paper_1.pdf\",\n",
            "      \"page_number\": 9,\n",
            "      \"section_title\": \"Possible Relevant Section Page 9\",\n",
            "      \"importance_rank\": 1\n",
            "    },\n",
            "    {\n",
            "      \"document\": \"paper_1.pdf\",\n",
            "      \"page_number\": 10,\n",
            "      \"section_title\": \"Possible Relevant Section Page 10\",\n",
            "      \"importance_rank\": 2\n",
            "    },\n",
            "    {\n",
            "      \"document\": \"paper_1.pdf\",\n",
            "      \"page_number\": 11,\n",
            "      \"section_title\": \"Possible Relevant Section Page 11\",\n",
            "      \"importance_rank\": 2\n",
            "    },\n",
            "    {\n",
            "      \"document\": \"paper_1.pdf\",\n",
            "      \"page_number\": 12,\n",
            "      \"section_title\": \"Possible Relevant Section Page 12\",\n",
            "      \"importance_rank\": 2\n",
            "    },\n",
            "    {\n",
            "      \"document\": \"paper_1.pdf\",\n",
            "      \"page_number\": 13,\n",
            "      \"section_title\": \"Possible Relevant Section Page 13\",\n",
            "      \"importance_rank\": 2\n",
            "    },\n",
            "    {\n",
            "      \"document\": \"paper_1.pdf\",\n",
            "      \"page_number\": 14,\n",
            "      \"section_title\": \"Possible Relevant Section Page 14\",\n",
            "      \"importance_rank\": 1\n",
            "    },\n",
            "    {\n",
            "      \"document\": \"paper_1.pdf\",\n",
            "      \"page_number\": 15,\n",
            "      \"section_title\": \"Possible Relevant Section Page 15\",\n",
            "      \"importance_rank\": 1\n",
            "    },\n",
            "    {\n",
            "      \"document\": \"paper_1.pdf\",\n",
            "      \"page_number\": 16,\n",
            "      \"section_title\": \"Possible Relevant Section Page 16\",\n",
            "      \"importance_rank\": 2\n",
            "    },\n",
            "    {\n",
            "      \"document\": \"paper_1.pdf\",\n",
            "      \"page_number\": 17,\n",
            "      \"section_title\": \"Possible Relevant Section Page 17\",\n",
            "      \"importance_rank\": 1\n",
            "    },\n",
            "    {\n",
            "      \"document\": \"paper_1.pdf\",\n",
            "      \"page_number\": 18,\n",
            "      \"section_title\": \"Possible Relevant Section Page 18\",\n",
            "      \"importance_rank\": 2\n",
            "    },\n",
            "    {\n",
            "      \"document\": \"paper_1.pdf\",\n",
            "      \"page_number\": 19,\n",
            "      \"section_title\": \"Possible Relevant Section Page 19\",\n",
            "      \"importance_rank\": 2\n",
            "    },\n",
            "    {\n",
            "      \"document\": \"paper_2.pdf\",\n",
            "      \"page_number\": 1,\n",
            "      \"section_title\": \"Possible Relevant Section Page 1\",\n",
            "      \"importance_rank\": 1\n",
            "    },\n",
            "    {\n",
            "      \"document\": \"paper_2.pdf\",\n",
            "      \"page_number\": 4,\n",
            "      \"section_title\": \"Possible Relevant Section Page 4\",\n",
            "      \"importance_rank\": 1\n",
            "    },\n",
            "    {\n",
            "      \"document\": \"paper_2.pdf\",\n",
            "      \"page_number\": 5,\n",
            "      \"section_title\": \"Possible Relevant Section Page 5\",\n",
            "      \"importance_rank\": 1\n",
            "    },\n",
            "    {\n",
            "      \"document\": \"paper_2.pdf\",\n",
            "      \"page_number\": 6,\n",
            "      \"section_title\": \"Possible Relevant Section Page 6\",\n",
            "      \"importance_rank\": 2\n",
            "    },\n",
            "    {\n",
            "      \"document\": \"paper_3.pdf\",\n",
            "      \"page_number\": 1,\n",
            "      \"section_title\": \"Possible Relevant Section Page 1\",\n",
            "      \"importance_rank\": 2\n",
            "    },\n",
            "    {\n",
            "      \"document\": \"paper_3.pdf\",\n",
            "      \"page_number\": 2,\n",
            "      \"section_title\": \"Possible Relevant Section Page 2\",\n",
            "      \"importance_rank\": 2\n",
            "    },\n",
            "    {\n",
            "      \"document\": \"paper_3.pdf\",\n",
            "      \"page_number\": 3,\n",
            "      \"section_title\": \"Possible Relevant Section Page 3\",\n",
            "      \"importance_rank\": 2\n",
            "    },\n",
            "    {\n",
            "      \"document\": \"paper_3.pdf\",\n",
            "      \"page_number\": 4,\n",
            "      \"section_title\": \"Possible Relevant Section Page 4\",\n",
            "      \"importance_rank\": 2\n",
            "    },\n",
            "    {\n",
            "      \"document\": \"paper_3.pdf\",\n",
            "      \"page_number\": 5,\n",
            "      \"section_title\": \"Possible Relevant Section Page 5\",\n",
            "      \"importance_rank\": 2\n",
            "    },\n",
            "    {\n",
            "      \"document\": \"paper_3.pdf\",\n",
            "      \"page_number\": 6,\n",
            "      \"section_title\": \"Possible Relevant Section Page 6\",\n",
            "      \"importance_rank\": 2\n",
            "    },\n",
            "    {\n",
            "      \"document\": \"paper_3.pdf\",\n",
            "      \"page_number\": 7,\n",
            "      \"section_title\": \"Possible Relevant Section Page 7\",\n",
            "      \"importance_rank\": 2\n",
            "    },\n",
            "    {\n",
            "      \"document\": \"paper_3.pdf\",\n",
            "      \"page_number\": 8,\n",
            "      \"section_title\": \"Possible Relevant Section Page 8\",\n",
            "      \"importance_rank\": 2\n",
            "    },\n",
            "    {\n",
            "      \"document\": \"paper_3.pdf\",\n",
            "      \"page_number\": 9,\n",
            "      \"section_title\": \"Possible Relevant Section Page 9\",\n",
            "      \"importance_rank\": 2\n",
            "    },\n",
            "    {\n",
            "      \"document\": \"paper_3.pdf\",\n",
            "      \"page_number\": 10,\n",
            "      \"section_title\": \"Possible Relevant Section Page 10\",\n",
            "      \"importance_rank\": 2\n",
            "    },\n",
            "    {\n",
            "      \"document\": \"paper_3.pdf\",\n",
            "      \"page_number\": 11,\n",
            "      \"section_title\": \"Possible Relevant Section Page 11\",\n",
            "      \"importance_rank\": 1\n",
            "    },\n",
            "    {\n",
            "      \"document\": \"paper_3.pdf\",\n",
            "      \"page_number\": 12,\n",
            "      \"section_title\": \"Possible Relevant Section Page 12\",\n",
            "      \"importance_rank\": 1\n",
            "    },\n",
            "    {\n",
            "      \"document\": \"paper_3.pdf\",\n",
            "      \"page_number\": 13,\n",
            "      \"section_title\": \"Possible Relevant Section Page 13\",\n",
            "      \"importance_rank\": 2\n",
            "    },\n",
            "    {\n",
            "      \"document\": \"paper_3.pdf\",\n",
            "      \"page_number\": 15,\n",
            "      \"section_title\": \"Possible Relevant Section Page 15\",\n",
            "      \"importance_rank\": 2\n",
            "    },\n",
            "    {\n",
            "      \"document\": \"paper_3.pdf\",\n",
            "      \"page_number\": 16,\n",
            "      \"section_title\": \"Possible Relevant Section Page 16\",\n",
            "      \"importance_rank\": 2\n",
            "    },\n",
            "    {\n",
            "      \"document\": \"paper_3.pdf\",\n",
            "      \"page_number\": 18,\n",
            "      \"section_title\": \"Possible Relevant Section Page 18\",\n",
            "      \"importance_rank\": 2\n",
            "    },\n",
            "    {\n",
            "      \"document\": \"paper_3.pdf\",\n",
            "      \"page_number\": 19,\n",
            "      \"section_title\": \"Possible Relevant Section Page 19\",\n",
            "      \"importance_rank\": 2\n",
            "    }\n",
            "  ],\n",
            "  \"subsection_analysis\": [\n",
            "    {\n",
            "      \"document\": \"paper_1.pdf\",\n",
            "      \"page_number\": 1,\n",
            "      \"refined_text\": \"OGB-LSC: A Large-Scale Challenge for\\nMachine Learning on Graphs\\nWeihua Hu1, Matthias Fey2, Hongyu Ren1, Maho Nakata3, Yuxiao Dong4, Jure Leskovec1\\n1Department of Computer Science, Stanford University\\n2Department of Computer Science, TU Dortmund University\\n3RIKEN, 4Facebook AI\\nogb-lsc@cs.stanford.edu\\nAbstract\\nEnabling effective and ef\\ufb01cient machine learning (ML) over large-scale graph\\ndata (e.g., graphs with billions of edges) can have a great impact on both indus-\\ntrial and scienti\\ufb01c applications. However, existing efforts to advance large-scale\\ngraph ML have been largely limited by the lack of a suitable public benchmark. Here we present OGB Large-Scale Challenge (OGB-LSC), a collection of three\\nreal-world datasets for facilitating the advancements in large-scale graph ML.\"\n",
            "    },\n",
            "    {\n",
            "      \"document\": \"paper_1.pdf\",\n",
            "      \"page_number\": 2,\n",
            "      \"refined_text\": \"Node-level\\nMAG240M-LSC\\nPaper\\nAuthor\\nInstitution\\nwrites\\naffiliated with\\ncites\\nSubject area? Predict\\n(a) MAG240M\\nLink-level\\nWikiKG90M-LSC\\nis a\\nGeoffrey Hinton\\nPerson\\naffiliated \\nwith\\nUniversity of \\nToronto\\nCanada\\nlocated in\\nPaul Martin\\ngraduated \\nfrom\\nborn in\\nGraduated \\nfrom\\n? King\\u2019s College, \\nCambridge\\nPredict\\n(b) WikiKG90M\\nGraph-level\\nPCQM4M-LSC\\nQuantum \\ncalculation (DFT)\\n~hours\\nHOMO-LUMO gap?\"\n",
            "    },\n",
            "    {\n",
            "      \"document\": \"paper_1.pdf\",\n",
            "      \"page_number\": 3,\n",
            "      \"refined_text\": \"gains2, further solidifying the value of the OGB-LSC datasets to advance state-of-the-art. We\\nsummarize the common techniques shared by the winning solutions, highlighting the current best\\npractices of large-scale graph ML. Moreover, based on the lessons learned from the KDD Cup, we\\ndescribe the future plan to update the datasets so that they can be further used to advance large-scale\\ngraph ML.\"\n",
            "    },\n",
            "    {\n",
            "      \"document\": \"paper_1.pdf\",\n",
            "      \"page_number\": 4,\n",
            "      \"refined_text\": \"citation connection (i.e., P-P), certain meta-paths (i.e., P-A-P) give rise to much higher degrees of\\nhomophiliness, while other meta-paths (i.e., P-A-I-A-P) provide much less homophily. As homophily\\nis the central graph property exploited by many graph ML models, we believe that discovering\\nessential heterogeneous connectivity is important to achieve good performance on this dataset. Dataset split.\"\n",
            "    },\n",
            "    {\n",
            "      \"document\": \"paper_1.pdf\",\n",
            "      \"page_number\": 5,\n",
            "      \"refined_text\": \"Paper\\n121,751,666 nodes\\nAuthor\\n122,383,112 nodes\\nInstitution\\n25,721 nodes\\naffiliated with\\n44,592,586 edges\\nwrites\\n386,022,720 edges\\ncites\\n1,297,748,926 edges\\nFigure 2: A schema diagram of MAG240M. Table 2: Results of MAG240M measured by\\nthe accuracy (%). Model\\n#Params Validation Test\\nMLP\\n0.5M\\n52.67\\n52.73\\nLABELPROP\\n0\\n58.44\\n56.29\\nSGC\\n0.7M\\n65.82\\n65.29\\nSIGN\\n3.8M\\n66.64\\n66.09\\nMLP+C&S\\n0.5M\\n66.98\\n66.18\\nGRAPHSAGE (NS)\\n4.9M\\n66.79\\n66.28\\nGAT (NS)\\n4.9M\\n67.15\\n66.80\\nR-GRAPHSAGE (NS)\\n12.2M\\n69.86\\n68.94\\nR-GAT (NS)\\n12.3M\\n70.02\\n69.42\\nKDD 1ST: BD-PGL\\n75.49\\nKDD 2ND: ACADEMIC\\n75.19\\nKDD 3RD: SYNERISE AI\\n74.60\\nTable 3: Analysis of graph homophily for differ-\\nent meta-paths connecting 1,251,341 arXiv pa-\\npers (only train+validation).\"\n",
            "    },\n",
            "    {\n",
            "      \"document\": \"paper_1.pdf\",\n",
            "      \"page_number\": 6,\n",
            "      \"refined_text\": \"Prediction task and evaluation metric. The task is the KG completion, i.e., given a set of training\\ntriplets, predict a set of test triplets. For evaluation, we follow the protocol similar to how KG\\ncompletion is evaluated (Bordes et al., 2013).\"\n",
            "    },\n",
            "    {\n",
            "      \"document\": \"paper_1.pdf\",\n",
            "      \"page_number\": 7,\n",
            "      \"refined_text\": \"Table 4:\\nTextual representation of validation\\ntriplets whose head entities only appear once as\\nhead in the training WikiKG90M. Head\\nRelation\\nTail\\nFood and drink companies of Bulgaria combines topics Bulgaria\\nPerforming arts in Denmark\\ncombines topics performing arts\\nAnglicanism in Grenada\\ncombines topics Anglicanism\\nChuan Li\\noccupation\\nresearcher\\nPetra Junkova\\ngiven name\\nPetra\\nTable 5: Results of WikiKG90M measured\\nby Mean Reciprocal Rank (MRR). Model\\n#Params Validation\\nTest\\nTRANSE-SHALLOW\\n17.4B\\n0.7559\\n0.7412\\nCOMPLEX-SHALLOW\\n17.4B\\n0.6142\\n0.5883\\nTRANSE-ROBERTA\\n0.3M\\n0.6039\\n0.6288\\nCOMPLEX-ROBERTA\\n0.3M\\n0.7052\\n0.7186\\nTRANSE-CONCAT\\n17.4B\\n0.8494\\n0.8548\\nCOMPLEX-CONCAT\\n17.4B\\n0.8425\\n0.8637\\nKDD 1ST: BD-PGL\\n0.9727\\nKDD 2ND: OHMYGOD\\n0.9712\\nKDD 3RD: GRAPHMIRACLES\\n0.9707\\nachieving near-perfect test MRR score of 0.97.\"\n",
            "    },\n",
            "    {\n",
            "      \"document\": \"paper_1.pdf\",\n",
            "      \"page_number\": 8,\n",
            "      \"refined_text\": \"Graph. We provide molecules as the SMILES strings (Weininger, 1988), from which 2D molecule\\ngraphs (nodes are atoms and edges are chemical bonds) as well as molecular \\ufb01ngerprints (hand-\\nengineered molecular feature developed by the chemistry community) can be obtained. By default, we\\nfollow OGB (Hu et al., 2020a) to convert the SMILES string into a molecular graph representation,\\nwhere each node is associated with a 9-dimensional feature (e.g., atomic number, chirality) and\\neach edge comes with a 3-dimensional feature (e.g., bond type, bond stereochemistry), although the\\noptimal set of input graph features remains to be explored.\"\n",
            "    },\n",
            "    {\n",
            "      \"document\": \"paper_1.pdf\",\n",
            "      \"page_number\": 9,\n",
            "      \"refined_text\": \"Table 6: Results of PCQM4M measured by MAE\\n[eV]. The lower, the better. Ablation study of using\\nonly 10% of training data is also shown.\"\n",
            "    },\n",
            "    {\n",
            "      \"document\": \"paper_1.pdf\",\n",
            "      \"page_number\": 10,\n",
            "      \"refined_text\": \"Updates on leaderboards. We are introducing public leaderboards to facilitate further research\\nadvances after our KDD Cup. The test submissions of the KDD Cup 2021 were evaluated on the\\nentire hidden test set.\"\n",
            "    },\n",
            "    {\n",
            "      \"document\": \"paper_1.pdf\",\n",
            "      \"page_number\": 11,\n",
            "      \"refined_text\": \"Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. Freebase: a collabo-\\nratively created graph database for structuring human knowledge. In Special Interest Group on\\nManagement of Data (SIGMOD), pages 1247\\u20131250.\"\n",
            "    },\n",
            "    {\n",
            "      \"document\": \"paper_1.pdf\",\n",
            "      \"page_number\": 12,\n",
            "      \"refined_text\": \"William L Hamilton, Rex Ying, and Jure Leskovec. Inductive representation learning on large graphs. In Advances in Neural Information Processing Systems (NeurIPS), pages 1025\\u20131035, 2017.\"\n",
            "    },\n",
            "    {\n",
            "      \"document\": \"paper_1.pdf\",\n",
            "      \"page_number\": 13,\n",
            "      \"refined_text\": \"Christopher Morris, Nils M Kriege, Franka Bause, Kristian Kersting, Petra Mutzel, and Marion\\nNeumann. Tudataset: A collection of benchmark datasets for learning with graphs. arXiv preprint\\narXiv:2007.08663, 2020.\"\n",
            "    },\n",
            "    {\n",
            "      \"document\": \"paper_1.pdf\",\n",
            "      \"page_number\": 14,\n",
            "      \"refined_text\": \"Yizhou Sun, Jiawei Han, Xifeng Yan, Philip S Yu, and Tianyi Wu. Pathsim: Meta path-based top-k\\nsimilarity search in heterogeneous information networks. Proceedings of the VLDB Endowment, 4\\n(11):992\\u20131003, 2011.\"\n",
            "    },\n",
            "    {\n",
            "      \"document\": \"paper_1.pdf\",\n",
            "      \"page_number\": 15,\n",
            "      \"refined_text\": \"Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen,\\nand Tie-Yan Liu. Do transformers really perform bad for graph representation? arXiv preprint\\narXiv:2106.05234, 2021a.\"\n",
            "    },\n",
            "    {\n",
            "      \"document\": \"paper_1.pdf\",\n",
            "      \"page_number\": 16,\n",
            "      \"refined_text\": \"(c) Did you include any new assets either in the supplemental material or as a URL? [Yes]\\nAll of our relevant URLs are described in Appendix A. (d) Did you discuss whether and how consent was obtained from people whose data you\\u2019re\\nusing/curating?\"\n",
            "    },\n",
            "    {\n",
            "      \"document\": \"paper_1.pdf\",\n",
            "      \"page_number\": 17,\n",
            "      \"refined_text\": \"A\\nKey Information about OGB-LSC\\nDataset documentation. All of our datasets as well as how to use them through our Python package\\nare documented at https://ogb.stanford.edu/kddcup2021/. Our baseline code to repro-\\nduce all the results for each dataset is available at https://github.com/snap-stanford/\\nogb/tree/master/examples/lsc.\"\n",
            "    },\n",
            "    {\n",
            "      \"document\": \"paper_1.pdf\",\n",
            "      \"page_number\": 18,\n",
            "      \"refined_text\": \"Table 8: Basic graph statistics of the OGB-LSC datasets. The last three graph statistics are\\ncalculated over the \\u2018standardized\\u2019 graphs, where the graphs are \\ufb01rst converted into undirected and\\nunlabeled homogeneous graphs with duplicated edges removed. The SNAP library (Leskovec\\nand Sosi\\u02c7c, 2016) is then used to compute the graph statistics.\"\n",
            "    },\n",
            "    {\n",
            "      \"document\": \"paper_1.pdf\",\n",
            "      \"page_number\": 19,\n",
            "      \"refined_text\": \"entities that are not locations can be easily \\ufb01ltered out as negatives. Based on the the above intuition,\\nwe consider the relation-speci\\ufb01c tail candidate sets. Speci\\ufb01cally, on training triples, we pre-compute\\n20K most frequent tail entities for each relation and treat them as candidate tail entities for that\\nrelation.\"\n",
            "    },\n",
            "    {\n",
            "      \"document\": \"paper_2.pdf\",\n",
            "      \"page_number\": 1,\n",
            "      \"refined_text\": \"Recycling End\\u2013of\\u2013life\\nPolycarbonate in Steelmaking;\\nAb Initio Study of Carbon\\nDissolution in Molten Iron\\nM. Hussein N. Assadi* and Veena Sahajwalla\\nCentre for Sustainable Materials Research and Technology,\\nSchool of Materials Science and Engineering, The\\nUniversity of New South Wales, Sydney, 2052, Australia\\nCorresponding Author:\\n*Telephone: +61-2-9385-5234. E-mail:\\nHussein.assadi@unsw.edu.au\\nABSTRACT: The scarcity of fossil fuels as carbon\\nresources has motivated the steelmaking industry to\\nsearch  for  new  carbon  sources  such  as  end\\u2013of\\u2013life\\npolymeric  products. Using  ab  initio molecular\\ndynamics  simulation,  we  demonstrate  that  41%  of\\npolycarbonate\\u2019s carbon content is readily dissolved in\\nmolten  iron\\u2019s  interface  at  T =  1823  K  which  is\\ncomparable to graphite with  ~\\u200958% carbon content\\ndissolution.\"\n",
            "    },\n",
            "    {\n",
            "      \"document\": \"paper_2.pdf\",\n",
            "      \"page_number\": 4,\n",
            "      \"refined_text\": \"the cross\\u2013surface area of the supercell and  d to the\\nvolume  slab  with  the  height  of  1.5  \\u00c5 above  the\\nmolten  Fe  where  C\\u2013Fe  interaction  occur. Furthermore,\\n\\u03c1 s\\nand\\n\\u03c1 b\\n can readily be deduced\\nfrom the C concentration in the interface and bulk\\nareas. This  procedure  results  in  a  C  dissolution\\ncoefficient of 0.60  \\u00c52/ps for polycarbonate.\"\n",
            "    },\n",
            "    {\n",
            "      \"document\": \"paper_2.pdf\",\n",
            "      \"page_number\": 5,\n",
            "      \"refined_text\": \"(24)\\nHoover, W. G. Canonical Dynamics: Equilibrium \\nPhase-Space Distributions. Phys. Rev.\"\n",
            "    },\n",
            "    {\n",
            "      \"document\": \"paper_2.pdf\",\n",
            "      \"page_number\": 6,\n",
            "      \"refined_text\": \"Supporting Information \\nRecycling End\\u2013of\\u2013life Polycarbonate in Steelmaking; Ab \\nInitio Study of Carbon Dissolution in Molten Iron \\nM. H. N Assadi* and Veena Sahajwalla \\nCentre for Sustainable Materials Research and Technology, School of Materials Science \\nand Engineering, The University of New South Wales, Sydney, 2052, Australia \\nCorresponding Authors: \\n*Tel: +61-2-9385-5234. E-mail: Hussein.assadi@unsw.edu.au \\n \\nTo provide a comparative measure of polycarbonate\\u2019s performance as a carbon source, we \\nalso studied the dissolution of graphite in molten Fe. This is because graphite is a well\\u2013\\nstudied1 carburizing material and is known to have the highest carbon dissolution rate in \\nmolten Fe,2 thus it sets a benchmark for polycarbonate\\u2019s carburizing potentials.\"\n",
            "    },\n",
            "    {\n",
            "      \"document\": \"paper_3.pdf\",\n",
            "      \"page_number\": 1,\n",
            "      \"refined_text\": \"Situated and Interactive Multimodal Conversations\\nSeungwhan Moon\\u2217, Satwik Kottur\\u2217, Paul A. Crook\\u2020, Ankita De\\u2020, Shivani Poddar\\u2020\\nTheodore Levin, David Whitney, Daniel Difranco, Ahmad Beirami\\nEunjoon Cho, Rajen Subba, Alborz Geramifard\\nFacebook\\nB simmc@fb.com\\nAbstract\\nNext generation virtual assistants are envisioned to handle multimodal inputs (e.g., vision, mem-\\nories of previous interactions, and the user\\u2019s utterances), and perform multimodal actions (e.g.,\\ndisplaying a route while generating the system\\u2019s utterance). We introduce Situated Interactive\\nMultiModal Conversations (SIMMC) as a new direction aimed at training agents that take mul-\\ntimodal actions grounded in a co-evolving multimodal input context in addition to the dialog\\nhistory. We provide two SIMMC datasets totalling \\u223c13K human-human dialogs (\\u223c169K utter-\\nances) collected using a multimodal Wizard-of-Oz (WoZ) setup, on two shopping domains: (a)\\nfurniture \\u2013 grounded in a shared virtual environment; and (b) fashion \\u2013 grounded in an evolv-\\ning set of images.\"\n",
            "    },\n",
            "    {\n",
            "      \"document\": \"paper_3.pdf\",\n",
            "      \"page_number\": 2,\n",
            "      \"refined_text\": \"Figure 1: Illustration of a SIMMC dialog: a user and an assistant interact in a co-observed multimodal\\nenvironment for a shopping scenario. The dialog is grounded in an evolving multimodal context. The\\nground-truth of which items (e.g., prefabs) appear is known for each view.\"\n",
            "    },\n",
            "    {\n",
            "      \"document\": \"paper_3.pdf\",\n",
            "      \"page_number\": 3,\n",
            "      \"refined_text\": \"Dataset\\nModality\\nTask\\nProvided Context\\nUpdated\\nAnnotation\\nQ\\u2019er\\nA\\u2019er\\nContext\\nGranularity\\nVisual Dialog (Das et al., 2017)\\nImage\\nQ&A\\nN/A\\nVisual\\nN/A\\nN/A\\nCLEVR-Dialog (Kottur et al., 2019)\\nSimulated\\nQ&A\\nN/A\\nVisual\\nN/A\\nN/A\\nGuessWhat (De Vries et al., 2017)\\nImage\\nQ&A\\nN/A\\nVisual\\nN/A\\nN/A\\nAudio Visual Scene-Aware Dialog (Hori et al., 2018)\\nVideo\\nQ&A\\nN/A\\nVisual\\nN/A\\nN/A\\nTalkTheWalk (de Vries et al., 2018)\\nImage\\nNavigation\\nVisual\\nVisual + Meta\\nLocation\\nU \\u2194A\\nVisual-Dialog Navigation (Thomason et al., 2019)\\nSimulated\\nNavigation\\nVisual\\nVisual + Meta\\nLocation\\nU \\u2194A\\nRelative Captioning (Guo et al., 2018)\\nImage\\nImage Retrieval\\nVisual\\nVisual + Meta\\nNew Image\\nU \\u2194A\\nMMD (Saha et al., 2018)\\nImage\\nImage Retrieval\\nVisual\\nVisual + Meta\\nNew Image\\nU \\u2194A\\nSIMMC (proposed)\\nImage/VR\\nTask-oriented\\nVisual\\nVisual + Meta\\nSituated\\nU \\u2194A + Semantic\\nTable 1: Comparison with the existing multimodal dialog corpora. Notations: (U \\u2194A) Utterance to\\naction pair labels. (Task-oriented) Includes API action prediction, Q&A, recommendation, item / image\\nretrieval and interaction.\"\n",
            "    },\n",
            "    {\n",
            "      \"document\": \"paper_3.pdf\",\n",
            "      \"page_number\": 4,\n",
            "      \"refined_text\": \"The SIMMC-Fashion (Image) Dataset represents user interactions with an assistant to obtain rec-\\nommendations for clothing items, e.g., jacket, dress. Conversations are grounded in real-world images\\nthat simulate a shopping scene from a user\\u2019s point-of-view (POV). At the start of each dialog the user is\\npresented with a randomly selected \\u2018seed\\u2019 image from the catalog to emulate (visually) that they are in\\nthe middle of shopping, as well as a sequence of synthetic memories of \\u2018previously viewed items\\u2019.\"\n",
            "    },\n",
            "    {\n",
            "      \"document\": \"paper_3.pdf\",\n",
            "      \"page_number\": 5,\n",
            "      \"refined_text\": \"4\\nSIMMC Dialog Annotations\\nBuilding a task-oriented multimodal conversational model introduces many new challenges, as it requires\\nboth action and item-level understanding of multimodal interactions. While most of the previous multi-\\nmodal datasets provide surface-level annotations (e.g., utterance to multimodal action pairs), we believe\\nit is critical to provide the semantic-level \\ufb01ne-grained annotations that ground the visual context, allow-\\ning for a more systematic and structural study for visual grounding of conversations. Towards this end,\\nwe develop a novel SIMMC ontology that captures the detailed multimodal interactions within dialog\\n\\ufb02ows.\"\n",
            "    },\n",
            "    {\n",
            "      \"document\": \"paper_3.pdf\",\n",
            "      \"page_number\": 6,\n",
            "      \"refined_text\": \"4.3\\nSIMMC Coreference Annotations\\nNote that the proposed labeling language allows for the annotation of object types in a dialog, which\\nmay in turn refer to speci\\ufb01c canonical listings from the underlying multimodal contexts. For example,\\ngiven an annotated utterance \\u201c[DA:REQUEST:GET:CHAIR Show me the back of it]\\u201d, the annotated object\\n\\u2018CHAIR\\u2019 (it) would refer to a speci\\ufb01c catalog item, represented as a item id within the image metadata. To\\nallow for structural grounding between the verbal and visual modalities in a shared catalog, we further\\nannotate the mapping of object type mentions in the annotated utterance to the corresponding item id\\nin the image metadata.\"\n",
            "    },\n",
            "    {\n",
            "      \"document\": \"paper_3.pdf\",\n",
            "      \"page_number\": 7,\n",
            "      \"refined_text\": \"API with the price argument. A comprehensive set of APIs for our SIMMC dataset is given in Tab. 8.\"\n",
            "    },\n",
            "    {\n",
            "      \"document\": \"paper_3.pdf\",\n",
            "      \"page_number\": 8,\n",
            "      \"refined_text\": \"2017) that achieved state-of-the-art results in language modeling (Devlin et al., 2019). Multimodal Fusion. This component fuses semantic information from the text (ut and ht) and the\\nmultimodal context Mt (described in Sec.\"\n",
            "    },\n",
            "    {\n",
            "      \"document\": \"paper_3.pdf\",\n",
            "      \"page_number\": 9,\n",
            "      \"refined_text\": \"be represented as the concatenation of input and target output yt = [Ht; Mt; Ut; bt], where both Mt and\\nbt are cast as string tokens of key value pairs. The language model is then \\ufb01ne-tuned to learn the joint\\nprobability with p(xt) = Qn\\ni=1 p(xt,i|xt,<i) for all n tokens in a sequence. At inference time, we provide\\nthe user input context xt = [Ht; Mt; Ut] as a seed for the language model, and parse the generated output\\nto obtain the structural representation of user belief states.\"\n",
            "    },\n",
            "    {\n",
            "      \"document\": \"paper_3.pdf\",\n",
            "      \"page_number\": 10,\n",
            "      \"refined_text\": \"Model\\nTask 1. API Prediction\\nTask 2. Response Generation\\nAcc\\u2191\\nPerp\\u2193\\nA.Acc\\u2191\\nBLEU\\u2191\\nr@1\\u2191\\nr@5\\u2191\\nr@10\\u2191\\nMean\\u2193\\nMRR\\u2191\\nSIMMC-Furniture\\nTF-IDF\\n77.1\\n2.59\\n57.5\\n-\\n-\\n-\\n-\\n-\\n-\\nLSTM\\n-\\n-\\n-\\n0.022\\n4.1\\n11.1\\n17.3\\n46.4\\n0.094\\nHAE\\n79.7\\n1.70\\n53.6\\n0.075\\n12.9\\n28.9\\n38.4\\n31.0\\n0.218\\nHRE\\n80.0\\n1.66\\n54.7\\n0.075\\n13.8\\n30.5\\n40.2\\n30.0\\n0.229\\nMN\\n79.2\\n1.71\\n53.3\\n0.084\\n15.3\\n31.8\\n42.2\\n29.1\\n0.244\\nT-HAE\\n78.4\\n1.83\\n53.6\\n0.044\\n8.5\\n20.3\\n28.9\\n37.9\\n0.156\\nSTOD++\\u2020 72.2\\n-\\n61.4\\n0.155\\n-\\n-\\n-\\n-\\n-\\nSIMMC-Fashion\\nTD-IDF\\n78.1\\n3.51\\n57.9\\n-\\n-\\n-\\n-\\n-\\n-\\nLSTM\\n-\\n-\\n-\\n0.022\\n5.3\\n11.4\\n16.5\\n46.9\\n0.102\\nHAE\\n81.0\\n1.75\\n60.2\\n0.059\\n10.5\\n25.3\\n34.1\\n33.5\\n0.190\\nHRE\\n81.9\\n1.76\\n62.1\\n0.079\\n16.3\\n33.1\\n41.7\\n27.4\\n0.253\\nMN\\n81.6\\n1.74\\n61.6\\n0.065\\n16.1\\n31.0\\n39.4\\n29.3\\n0.245\\nT-HAE\\n81.4\\n1.78\\n62.1\\n0.051\\n10.3\\n23.2\\n31.1\\n37.1\\n0.178\\nTable 4: Results for: (1) API prediction via accuracy, perplexity\\nand attribute accuracy, and, (2) Response generation via BLEU,\\nrecall@k (k=1,5,10), mean rank, and mean reciprocal rank\\n(MRR).\"\n",
            "    },\n",
            "    {\n",
            "      \"document\": \"paper_3.pdf\",\n",
            "      \"page_number\": 11,\n",
            "      \"refined_text\": \"References\\nStanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick, and Devi\\nParikh. 2015. VQA: Visual question answering.\"\n",
            "    },\n",
            "    {\n",
            "      \"document\": \"paper_3.pdf\",\n",
            "      \"page_number\": 12,\n",
            "      \"refined_text\": \"Satwik Kottur, Jos\\u00b4e MF Moura, Devi Parikh, Dhruv Batra, and Marcus Rohrbach. 2019. Clevr-dialog: A diagnos-\\ntic dataset for multi-round reasoning in visual dialog.\"\n",
            "    },\n",
            "    {\n",
            "      \"document\": \"paper_3.pdf\",\n",
            "      \"page_number\": 13,\n",
            "      \"refined_text\": \"Figure 4: Distribution of Dialog Acts and Activities in the SIMMC datasets. See Sec. A for details.\"\n",
            "    },\n",
            "    {\n",
            "      \"document\": \"paper_3.pdf\",\n",
            "      \"page_number\": 15,\n",
            "      \"refined_text\": \"CLOTHING\\nageRange, amountInStock, availableSizes, brand, clothingCategory, cloth-\\ningStyle, color, condition, customerRating, embellishment, forGender, forOc-\\ncasion, forSeason, itemDescription, madeIn, material, ordinal, pattern, price,\\nsequential, size, items, soldBy, warmthRating, waterResistance\\nCOMPANY\\nheadquarteredIn, name*, ordinal, sequential\\nDATE TIME\\ndate, month, time, week, weekday, year\\nDISPLAY\\ndisplayPostion (displayFirst, displaySecond, displayThird)\\nDRESS\\ndressStyle, hemLength, hemStyle, necklineStyle, sleeveLength, sleeveStyle,\\nwaistStyle\\nEVENT\\nduration, elapsedTime, endTime, eventType, hasPart, name, remainingTime,\\nstartTime\\nFURNITURE\\nageRange, amountInStock, assemblyRequired, brand, color, condition, cur-\\nrentLocation, customerRating, decorStyle, dimensions (width, depth, height)\\nera, \\ufb01lling, \\ufb01nish, foldable, hasStorage, intendedRoom, isAdjustable, isAn-\\ntique, isVintage, madeIn, material, name, ordinal, owner, pattern, price, se-\\nquential , soldBy, swivels, upholstery, weight, weightCapacity\\nHOLIDAY\\nduration, endTime, name, startTime\\nJACKET\\nhemLength, hemStyle, jacketStyle, necklineStyle, sleeveLength, sleeveStyle,\\nwaistStyle\\nLOCATION\\ncity, continent, country, currentDate, currentTime, region, state\\nSITUATION\\nagent, situationLocation, situationTime, situationType, theme\\nSIZE\\nageSize, alphabeticSize, numericSize, ordinal, sequential, sizeType\\nSKIRT\\nhemLength, hemStyle, skirtStyle, waistStyle\\nSWEATER\\nnecklineStyle, sleeveLength, sleeveStyle, sweaterStyle, waistStyle\\nUSER\\nattentionOn, name\\nTable 6: List of object attributes in the SIMMC ontology\\nCHECK\\ncheck (STRING)\\nCOMPARE\\ncomp (OBJECT)\\nCOUNT\\ncountFrom (THING), countTo (THING), countUnit (STRING)\\nTable 7: List of activity attributes in the SIMMC ontology\\nD\\nAPI Call List\\nTab. 8 shows the list of all APIs supported in our SIMMC datasets. E\\nImplementation Details\\nAll our models are trained using PyTorch (Paszke et al., 2019).\"\n",
            "    },\n",
            "    {\n",
            "      \"document\": \"paper_3.pdf\",\n",
            "      \"page_number\": 16,\n",
            "      \"refined_text\": \"API Name\\nArguments\\nSIMMC-Furniture\\nSearchFurniture:\\nSearch items using the item attributes\\nCategory, color, intended room, material, price\\nrange, etc. SpecifyInfo:\\nGet and specify information (attributes) about an item\\nMaterial, price range (min\\u2013max), customer rating,\\netc. FocusOnFurniture:\\nFocus on an item to enlarge (for a better view)\\nPosition of argument item on the carousel (left, cen-\\nter, right)\\nRotateFurniture:\\nRotate a focused furniture item in the view\\nRotational directions (left, right, up, down, front,\\nback)\\nNavigateCarousel:\\nNavigate the carousel to explore search results\\nNavigating directions (next and previous)\\nSIMMC-Fashion\\nSpecifyInfo:\\nGet and specify information (attributes) about an item\\nBrand, price, customer rating, available sizes, col-\\nors, etc.\"\n",
            "    },\n",
            "    {\n",
            "      \"document\": \"paper_3.pdf\",\n",
            "      \"page_number\": 18,\n",
            "      \"refined_text\": \"Situated Context\\nDialog Utterances\\nDialog Annotation\\nU: I am looking for table\\nlamps\\n[DA:REQUEST:GET:TABLE LAMP I\\nam look for table lamps]\\nA: Are you looking for\\nbedroom lamps? [DA:PROMPT:REFINE:TABLE LAMP\\n.intendedRoom Are you looking for [.in-\\ntendedRoom bedroom] lamps?] U: Yes, bedroom lamps\\n[DA:INFORM:REFINE:TABLE LAMP\\n.intendedRoom\\nYes,\\n[.intendedRoom\\nbedroom] lamps]\\nA: If you\\u2019re into a so-\\nphisticated style, this neu-\\ntral gray \\ufb01nish lamp will\\n\\ufb01t any color palette.\"\n",
            "    },\n",
            "    {\n",
            "      \"document\": \"paper_3.pdf\",\n",
            "      \"page_number\": 19,\n",
            "      \"refined_text\": \"Situated Context\\nDialog Utterances\\nDialog Annotation\\nU: What can you tell\\nme about the style of\\nthat brown skirt? [IN:ASK:GET:SKIRT.skirtStyle\\nWhat\\ncan you tell me about the style of\\n[USER.attentionOn this] skirt?] A: This style is loose ball\\ngown.\"\n",
            "    }\n",
            "  ]\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TtRjZvkJg2Ln"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyOCvaQVQYkLacF3xZeWYd6b",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}